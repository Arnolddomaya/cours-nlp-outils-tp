{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams avec NLTK : Génération de texte\n",
    "\n",
    "\n",
    "L'objectif de ce notebook est d'arriver à générer automatiquement des phrases ayant un maximum de sens. Pour cela, nous allons appliquer la notion de n-gram. L'application ici va uniquement se baser sur les probabilités pour un mot d'apparaître en suivant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 1 : Récupération du texte\n",
    "\n",
    "Dans le dossier `data` à la racine du projet, vous trouverez quelques textes issus du projet gutenberg avec lesquels nous allons pouvoir travailler.\n",
    "\n",
    "La première étape consiste à lire ces texte et les concatener dans une variable python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "for filepath in glob.glob(\"../data/fr_*.txt\"):\n",
    "    with open(filepath, encoding='utf-8') as file:\n",
    "        ## ----- TODO : Lire le texte et le concatener dans la variable text ----- ##\n",
    "        ## ----------------------------------------------------------------------- ##\n",
    "        text += \"\\n\"\n",
    "\n",
    "assert \"Le Chat grimaça en apercevant Alice\" in text, \"Vous devez charger le contenu du livre dans la variable.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 2 : Génération des trigrams\n",
    "\n",
    "Les trigrams sont toutes les suites de 3 mots présents dans un texte : https://en.wikipedia.org/wiki/Trigram. NLTK propose une fonction toute prête pour les calculer que nous allons appeler.\n",
    "\n",
    "Une fois ces trigrams obtenus, nous allons construire un dictionnaire python (ou une autre forme, à vous de voir ce que vous préferez). Ce dictionnaire contiendra pour chaque mot, la liste des mots qui le suivent, et pour chacucun de ces mots, encore une fois, la liste des mots qui le suivent.  On pourra ainsi avoir les suites de mots sur 3 niveaux et calculer les frequences d'apparitions de chaque mots en fonction des deux précedents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199856\n",
      "-----------\n",
      "[('erreurs', 'clairement', 'introduites'), ('clairement', 'introduites', 'par'), ('introduites', 'par', 'le'), ('par', 'le', 'typographe'), ('le', 'typographe', 'ont')]\n",
      "-----------\n",
      "('électronique', 'reproduit', 'intégralement')\n",
      "-----------\n",
      "{'reproduit': ['intégralement', 'dans', 'dans']}\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import trigrams\n",
    "\n",
    "# Tokenize text with nltk\n",
    "text = text.lower()\n",
    "words = word_tokenize(text)\n",
    "\n",
    "# Calculate trigrams with nltk trigrams function\n",
    "triplets = list(trigrams(words))\n",
    "\n",
    "## ----- TODO : Construire un dictionnaire de dictionnaires de listes ----- ##\n",
    "## ------------------------------------------------------------------------ ##\n",
    "\n",
    "assert type(data) == type({})\n",
    "assert type(data['le']) == type({})\n",
    "assert type(data['le']['livre']) == type([])\n",
    "assert 'de' in data['le']['livre']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 3 : Calcul des probabilités associées à chaque mot\n",
    "\n",
    "Nous avons besoin d'une fonction qui, lorsqu'on lui donne les deux premiers mots, va nous sortir la liste des possibilités pour le troisième mot. Grâce à cette liste, nous allons pouvoir en selectionner un au hasard parmis les plus probables et construire notre phrase.\n",
    "\n",
    "Par exemple, `get_top_third_words(\"un\", \"grand\", 5)` retourne : \n",
    "```python\n",
    "[('bruit', 0.04878048780487805),\n",
    " ('nombre', 0.04878048780487805),\n",
    " ('fracas', 0.04878048780487805),\n",
    " ('poltron', 0.04878048780487805),\n",
    " ('cercle', 0.04878048780487805)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nombre', 0.04878048780487805),\n",
       " ('bruit', 0.04878048780487805),\n",
       " ('cercle', 0.04878048780487805),\n",
       " ('poltron', 0.04878048780487805),\n",
       " ('fracas', 0.04878048780487805)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_third_word_probabilities(first_word, second_word):\n",
    "    ## ----- TODO : Retourner la liste des mots avec les probabilités associées ----- ##\n",
    "    ## -------------------------------------------------------------------------------##\n",
    "    return probas\n",
    "\n",
    "def get_top_third_words(first_word, second_word, top_number):\n",
    "    probas = get_third_word_probabilities(first_word, second_word)\n",
    "    sorted_probas = sorted(probas.items(), key=lambda kv: -kv[1])\n",
    "    return sorted_probas[:top_number]\n",
    "\n",
    "assert type(get_top_third_words(\"un\", \"grand\", 5)[0][0]) == type(\"\")\n",
    "assert type(get_top_third_words(\"un\", \"grand\", 5)[0][1]) == type(0.1)\n",
    "get_top_third_words(\"un\", \"grand\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Étape 4 : Génération automatique de texte\n",
    "\n",
    "Maintenant que nous pouvons obtenir le troisième mot à partir des deux premiers, on doit pouvoir construire une histoire. \n",
    "\n",
    "Écrire le code permettant, en prenant en entrée deux mots de notre vocabulaire, de générer un texte ayant du sens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce\n",
      "fut\n",
      "une\n",
      "sensation\n",
      "de\n",
      "mains\n",
      "sur\n",
      "baloo\n",
      "et\n",
      "toi\n",
      "pouvez\n",
      "vaquer\n",
      "sans\n",
      "nulle\n",
      "crainte\n",
      "à\n",
      "vos\n",
      "exemples\n",
      ",\n",
      "le\n",
      "singe\n",
      "de\n",
      "sa\n",
      "part\n",
      ";\n",
      "et\n",
      "les\n",
      "grenouilles\n",
      "qui\n",
      "demandent\n",
      "un\n",
      "roi\n"
     ]
    }
   ],
   "source": [
    "# ----- TODO : Générer du texte grâce à nos précedentes fonctions ----- ##\n",
    "# --------------------------------------------------------------------- ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Et maintenant ?\n",
    "\n",
    "Le texte pourrait très certainement être amelioré en augmentant le trigram en quadrigram... Nous aurions alors besoin de plus de texte d'entrainement !\n",
    "\n",
    "Maintenant nous allons utiliser NLTK et Scikit-learn pour entrainer notre modèle de Machine Learning et apprendre à reconnaître la langue d'un texte !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
